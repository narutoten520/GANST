{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stAA_default application"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main: run_stAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import squidpy as sq\n",
    "import scanpy as sc\n",
    "from sklearn.metrics.cluster import (\n",
    "    v_measure_score, homogeneity_score, completeness_score,  silhouette_score, homogeneity_completeness_v_measure, davies_bouldin_score)\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from train import stAA\n",
    "from Processdata import Adata2Torch_data, Spatial_Dis_Cal, process_adata, read_data, get_initial_label\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "gpu_id = 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "\n",
    "def make_dir(directory_path, new_folder_name):\n",
    "    \"\"\"Creates an expected directory if it does not exist\"\"\"\n",
    "    directory_path = os.path.join(directory_path, new_folder_name)\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    return directory_path\n",
    "\n",
    "\n",
    "def eval_embedding(pred, embedding=None):\n",
    "    sc = silhouette_score(embedding, pred, metric='euclidean')\n",
    "    db = davies_bouldin_score(embedding, pred)\n",
    "    return sc, db\n",
    "\n",
    "\n",
    "def run_stAA(adata, n_clusters, graph_mode=\"knn\", cluster_method=\"mclust\",\n",
    "             refine=True, data_save_path=\"./\", true_labels=None, eval=True):\n",
    "    # if cluster_method=\"louvain\", \"n_clusters\" represents the resolution\n",
    "    adata = process_adata(adata)\n",
    "\n",
    "    if graph_mode in [\"knn\", \"KNN\"]:\n",
    "        Spatial_Dis_Cal(adata, knn_dis=5, model=\"KNN\")\n",
    "    else:\n",
    "        Spatial_Dis_Cal(adata, rad_dis=graph_mode)\n",
    "\n",
    "    if 'Spatial_Net' not in adata.uns.keys():\n",
    "        # 验证是否存在Spatial_Net\n",
    "        raise ValueError(\n",
    "            \"Please Compute Spatial Network using Spatial_Dis_Cal function first!\")\n",
    "    # Process the data\n",
    "    data = Adata2Torch_data(adata)\n",
    "    ss_labels = get_initial_label(adata, method=cluster_method,\n",
    "                                  n_clusters=n_clusters)\n",
    "    reso = n_clusters\n",
    "    if cluster_method == \"mclust\":\n",
    "        n_clusters = n_clusters\n",
    "    else:\n",
    "        n_clusters = len(set(ss_labels))\n",
    "    model = stAA(input_dim=data.x.shape[1], epochs=1000,\n",
    "                 hidden_dim=256, embed_dim=128, n_clusters=n_clusters).cuda()\n",
    "    res = model.train_model(\n",
    "        data, method=cluster_method, refine=refine,\n",
    "        position=adata.obsm['spatial'], eval=eval, reso=reso,\n",
    "        ss_labels=ss_labels, data_save_path=data_save_path,\n",
    "        labels=true_labels)\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.neighbors\n",
    "import scipy.sparse as sp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import scanpy as sc\n",
    "import metis\n",
    "import squidpy as sq\n",
    "from sklearn.decomposition import PCA\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "\n",
    "\n",
    "def read_data(dataset, data_path='/home/sda1/'):\n",
    "    if dataset == \"STARmap\":\n",
    "        file_fold = data_path + str(dataset)\n",
    "        adata = sc.read(file_fold+\"/STARmap_20180505_BY3_1k.h5ad\")\n",
    "        adata.var_names_make_unique()\n",
    "        df_meta = pd.read_table(file_fold + '/Annotation_STARmap_20180505_BY3_1k.txt',\n",
    "                                sep='\\t', index_col=0)\n",
    "        adata.obs['ground_truth'] = adata.obs[\"label\"]\n",
    "        adata.obs['Annotation'] = df_meta.loc[adata.obs_names, 'Annotation'].values\n",
    "\n",
    "    if dataset == \"Breast_cancer\":\n",
    "        file_fold = data_path + str(dataset) #please replace 'file_fold' with the download path\n",
    "        adata = sc.read_visium(file_fold, count_file='filtered_feature_bc_matrix.h5',\n",
    "                               load_images=True)\n",
    "        adata.var_names_make_unique()\n",
    "        df_meta = pd.read_table(file_fold + '/metadata.tsv',sep='\\t',\n",
    "                                index_col=0)\n",
    "        adata.obs['ground_truth'] = df_meta.loc[adata.obs_names, 'ground_truth'].values\n",
    "\n",
    "    if dataset == \"Mouse_hippocampus\":\n",
    "        adata = sq.datasets.slideseqv2()\n",
    "        adata.var_names_make_unique()\n",
    "    \n",
    "    if dataset in [\"Mouse_embryo_E9_E1S1\", \"Mouse_embryo_E9_E2S1\",\n",
    "                   \"Mouse_embryo_E9_E2S2\", \"Mouse_embryo_E9_E2S3\",\n",
    "                   \"Mouse_embryo_E9_E2S4\"]:\n",
    "        file_fold = data_path + str(dataset)\n",
    "        adata = sc.read(file_fold+\"/MOSTA.h5ad\")\n",
    "        adata.var_names_make_unique()\n",
    "        adata.obs[\"ground_truth\"] = adata.obs[\"annotation\"]\n",
    "    \n",
    "    if dataset == \"Mouse_olfactory_slide_seqv2\":\n",
    "        adata = sc.read_h5ad(data_path+\"/\"+dataset+\"/tutorial3.h5ad\")\n",
    "\n",
    "    return adata\n",
    "\n",
    "\n",
    "def process_adata(adata):\n",
    "    adata.var_names_make_unique()\n",
    "    #Normalization\n",
    "    sc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=3000) ##3000高变基因；seurat_v3\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4) ##normalized data\n",
    "    sc.pp.log1p(adata)  #log-transformed data\n",
    "    adata = adata[:, adata.var['highly_variable']]\n",
    "    return adata\n",
    "\n",
    "\n",
    "def Adata2Torch_data(adata): \n",
    "    G_df = adata.uns['Spatial_Net'].copy() \n",
    "    spots = np.array(adata.obs_names) \n",
    "    spots_id_tran = dict(zip(spots, range(spots.shape[0]))) \n",
    "    G_df['Spot1'] = G_df['Spot1'].map(spots_id_tran) \n",
    "    G_df['Spot2'] = G_df['Spot2'].map(spots_id_tran) \n",
    "\n",
    "    G = sp.coo_matrix((np.ones(G_df.shape[0]), (G_df['Spot1'], G_df['Spot2'])), \n",
    "        shape=(adata.n_obs, adata.n_obs))\n",
    "\n",
    "    G = G + sp.eye(G.shape[0]) \n",
    "    edgeList = np.nonzero(G) \n",
    "\n",
    "    if type(adata.X) == np.ndarray:\n",
    "        data = Data(edge_index=torch.LongTensor(np.array(\n",
    "            [edgeList[0], edgeList[1]])), x=torch.FloatTensor(adata.X))  \n",
    "    else:\n",
    "        data = Data(edge_index=torch.LongTensor(np.array(\n",
    "            [edgeList[0], edgeList[1]])), x=torch.FloatTensor(adata.X.todense())) \n",
    "    data = train_test_split_edges(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def Spatial_Dis_Cal(adata, rad_dis=None, knn_dis=None, model='Radius', verbose=True):\n",
    "    \"\"\"\\\n",
    "    Calculate the spatial neighbor networks, as the distance between two spots.\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata:  AnnData object of scanpy package.\n",
    "    rad_dis:  radius distance when model='Radius' 半径\n",
    "    knn_dis:  The number of nearest neighbors when model='KNN' 邻居个数\n",
    "    model:\n",
    "        The network construction model. When model=='Radius', the spot is connected to spots whose distance is less than rad_dis. \n",
    "        When model=='KNN', the spot is connected to its first knn_dis nearest neighbors.\n",
    "    Returns\n",
    "    -------\n",
    "    The spatial networks are saved in adata.uns['Spatial_Net']\n",
    "    \"\"\"\n",
    "    assert(model in ['Radius', 'KNN', \"BallTree\"]) #断言语句，可以用来调试程序。\n",
    "    if verbose:\n",
    "        print('------Calculating spatial graph...')\n",
    "    coor = pd.DataFrame(adata.obsm['spatial']) #Spot 空间坐标\n",
    "    coor.index = adata.obs.index #df的index改为spot名称\n",
    "    # coor.columns = ['imagerow', 'imagecol']\n",
    "    coor.columns = ['Spatial_X', 'Spatial_Y'] #修改df的列名\n",
    "\n",
    "    if model == 'Radius':\n",
    "        nbrs = sklearn.neighbors.NearestNeighbors(radius=rad_dis).fit(coor)\n",
    "        distances, indices = nbrs.radius_neighbors(coor, return_distance=True)\n",
    "        # Find the neighbors within a given radius of a point or points, 返回每个Spot在给定半径中的邻居个数及距离。\n",
    "        # distances, indices的rows等于Spot的个数，即每个Spot都对应一个distance 和 index list.\n",
    "        KNN_list = []\n",
    "        for spot in range(indices.shape[0]):\n",
    "            KNN_list.append(pd.DataFrame(zip([spot]*indices[spot].shape[0], indices[spot], distances[spot]))) #每个spot的邻居编号；距离。\n",
    "    \n",
    "    if model == 'KNN':\n",
    "        nbrs = sklearn.neighbors.NearestNeighbors(n_neighbors=knn_dis+1).fit(coor)\n",
    "        distances, indices = nbrs.kneighbors(coor)\n",
    "        KNN_list = []\n",
    "        for spot in range(indices.shape[0]):\n",
    "            KNN_list.append(pd.DataFrame(zip([spot]*indices.shape[1],indices[spot,:], distances[spot,:])))\n",
    "\n",
    "    if model == \"BallTree\":\n",
    "        from sklearn.neighbors import BallTree\n",
    "        tree = BallTree(coor)\n",
    "        distances, ind = tree.query(coor, k=knn_dis+1)\n",
    "        indices = ind[:, 1:]\n",
    "        KNN_list=[]\n",
    "\n",
    "        for spot in range(indices.shape[0]):\n",
    "            KNN_list.append(pd.DataFrame(zip([spot]*indices.shape[1],indices[spot,:], distances[spot,:])))\n",
    "        # for node_idx in range(coor.shape[0]):\n",
    "        #     for j in np.arange(0, indices.shape[1]):\n",
    "        #         KNN_list.append(node_idx, indices[node_idx][j])\n",
    "\n",
    "    KNN_df = pd.concat(KNN_list) #变为dataframe格式。\n",
    "    KNN_df.columns = ['Spot1', 'Spot2', 'Distance']\n",
    "\n",
    "    Spatial_Net = KNN_df.copy()\n",
    "    Spatial_Net = Spatial_Net.loc[Spatial_Net['Distance']>0,]\n",
    "    id_spot_trans = dict(zip(range(coor.shape[0]), np.array(coor.index), )) #构建一个词典，\n",
    "    Spatial_Net['Spot1'] = Spatial_Net['Spot1'].map(id_spot_trans) #Spot1的编号，e.g. spot1出现几次，表明有几个邻居，在spot2里。\n",
    "    Spatial_Net['Spot2'] = Spatial_Net['Spot2'].map(id_spot_trans) #Spot2的编号 spot1对应的邻居编号\n",
    "    if verbose:\n",
    "        print('The graph contains %d edges, %d spots.' %(Spatial_Net.shape[0], adata.n_obs)) #共多少条边\n",
    "        print('%.4f neighbors per spot on average.' %(Spatial_Net.shape[0]/adata.n_obs)) #平均每个Spot多少条边 \n",
    "    adata.uns['Spatial_Net'] = Spatial_Net\n",
    "\n",
    "\n",
    "def get_initial_label(adata, n_clusters, refine=True, method=\"mclust\"):\n",
    "    features = adata.X\n",
    "    if type(features) == np.ndarray:\n",
    "        features = features\n",
    "    else:\n",
    "        features = features.todense()\n",
    "    features = np.asarray(features)\n",
    "    pca_input = dopca(features, dim = 20) # dim 10-45 30 is the best\n",
    "    if method == \"mclust\":\n",
    "        pred = mclust_R(embedding=pca_input, num_cluster=n_clusters)\n",
    "    if method == \"louvain\":\n",
    "        adata.obsm[\"pca\"] = pca_input\n",
    "        sc.pp.neighbors(adata, n_neighbors=50, use_rep=\"pca\")\n",
    "        sc.tl.louvain(adata, resolution=n_clusters, random_state=0)\n",
    "        pred=adata.obs['louvain'].astype(int).to_numpy()\n",
    "    if refine:\n",
    "        pred = refine_label(pred, adata.obsm[\"spatial\"], radius=60)\n",
    "    pred = list(map(int, pred))\n",
    "    return np.array(pred)\n",
    "\n",
    "\n",
    "def dopca(X, dim=10):\n",
    "    pcaten = PCA(n_components=dim, random_state=42)\n",
    "    X_10 = pcaten.fit_transform(X)\n",
    "    return X_10\n",
    "\n",
    "\n",
    "def mclust_R(embedding, num_cluster, modelNames='EEE', random_seed=0):\n",
    "    \"\"\"\\\n",
    "    Clustering using the mclust algorithm.\n",
    "    The parameters are the same as those in the R package mclust.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    import rpy2.robjects as robjects\n",
    "    robjects.r.library(\"mclust\")\n",
    "    import rpy2.robjects.numpy2ri\n",
    "    rpy2.robjects.numpy2ri.activate()\n",
    "    r_random_seed = robjects.r['set.seed']\n",
    "    r_random_seed(random_seed)\n",
    "    rmclust = robjects.r['Mclust']\n",
    "    res = rmclust(rpy2.robjects.numpy2ri.numpy2rpy(\n",
    "        embedding), num_cluster, modelNames)\n",
    "    mclust_res = np.array(res[-2])\n",
    "\n",
    "    mclust_res = mclust_res.astype('int')\n",
    "    # mclust_res = mclust_res.astype('category')\n",
    "    return mclust_res\n",
    "\n",
    "def refine_label(label, position, \n",
    "                 radius=50):\n",
    "    new_type = []\n",
    "\n",
    "    # calculate distance\n",
    "    distance = ot.dist(position, position, metric='euclidean')\n",
    "\n",
    "    n_cell = distance.shape[0]\n",
    "\n",
    "    for i in range(n_cell):\n",
    "        vec = distance[i, :]\n",
    "        index = vec.argsort()\n",
    "        neigh_type = []\n",
    "        for j in range(1, radius+1):\n",
    "            neigh_type.append(label[index[j]])\n",
    "        max_type = max(neigh_type, key=neigh_type.count)\n",
    "        new_type.append(max_type)\n",
    "\n",
    "    new_type = [str(i) for i in list(new_type)]\n",
    "    # adata.obs['label_refined'] = np.array(new_type)\n",
    "\n",
    "    return new_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "file_fold = '/home/tengliu/Paper6-NC/GraphST/data/Tutorial1' #please replace 'file_fold' with the download path\n",
    "adata = sc.read_visium(file_fold, count_file='filtered_feature_bc_matrix.h5', load_images=True)\n",
    "adata.var_names_make_unique()\n",
    "# add ground_truth\n",
    "df_meta = pd.read_csv(file_fold + '/metadata.tsv', sep='\\t')\n",
    "df_meta_layer = df_meta['layer_guess']\n",
    "adata.obs['ground_truth'] = df_meta_layer.values\n",
    "\n",
    "# Run device, by default, the package is implemented on 'cpu'. We recommend using GPU.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# the location of R, which is necessary for mclust algorithm. Please replace the path below with local R installation path\n",
    "os.environ['R_HOME'] =\"/home/tengliu/miniconda3/envs/R41/lib/R\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_stAA(adata, n_clusters=7, cluster_method=\"mclust\", refine=False,\n",
    "               graph_mode=150, eval=True, data_save_path=\"./\",true_labels=adata.obs[\"ground_truth\"])\n",
    "print(res[\"embedding\"])\n",
    "print(res[\"pred_label\"])\n",
    "print(res[\"ari\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = adata.X\n",
    "if type(features) == np.ndarray:\n",
    "    features = features\n",
    "else:\n",
    "    features = features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spatial_Dis_Cal(adata, knn_dis=5, model=\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Adata2Torch_data(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data #[2, 15470]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_pos_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfrom = RandomLinkSplit(is_undirected=True)\n",
    "train_data, val_data, test_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train model and cluster\n",
    "res = run_stAA(adata, n_clusters=7,\n",
    "               cluster_method=\"mclust\", refine=False,\n",
    "               graph_mode=40, eval=False,\n",
    "               data_save_path=\"./\")\n",
    "print(res[\"embedding\"])\n",
    "print(res[\"pred_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_pyG2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
